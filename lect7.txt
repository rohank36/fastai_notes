FastAI Lecture 7: Collab Filtering

- More parameters => more memory used up on your gpu while training 
- Gradient Accumulation: 
	- a way of efficiently using the gpu by training on smaller batch sizes until the total count is > the target batch size
		then substraacting the gradient*lr
	- allows us to train larger models on less powerful gpus 
	- pick batch size that is a multiple of 8 (jeremy's rule of thumb)

- Multi Target model: 
- Cross Entropy Loss: tells how close the models preds are to actual pred and if model confidently making correct preds
	- You're always going have to care about what happens in the 1st and last layer of a NN
	- First thing you have to do is calculate the Softmax or Log Softmax
	- Softmax to CrossEntropy
		- Creating a probability distribution for each potential cateogry. Sum of all probs = 1
		- One hot encoding the probability distribution => this gives us the "actuals" column
		- For each category sum the actual * log of the p(actual) 
			- the p(actual) is just the probability distribution value 
			- only need to take log if you didn't use Log Softmax

	-Binary Cross Entropy: use when you are predicting if something is or is not (e.g is it a cat or not?)
		- Considers P(x) and 1-P(x) and takes P(x)*log(P(x)) and P(x)*log(1-P(x)) - refer to actual equation

Collaborative Filtering
- Latent Factors: we don't know what factors matter, but there is probably something that does matter. Lets try using SGD to find them. Latent = hidden or dormant.
	- init x random factors for each user and each target (e.g. movies)
	- maxtrix multiply to get the prediction
	- calculate loss on preds vs actual (e.g. pred movie rating vs actual movie rating)
	- SGD => optimized weights
	- Preds that come close to the actual but now we have learned factors (weights)
	- Choosing number of factors: fastai has defaults, obscure art. Based on Jeremy's own intution.
	- Have to add the bias (1 for the movie embedding and one for the user embedding) to the sum of the mm between the user and movie embeddings
- Embedding: like looking up something in an array (computational shortcut, like vector @ one-hot encoded vector, but instead you're just looking up 
	the pos in an array. They do the same thing)
- Note: if the loss gets better then gets worse, it may be an indication of overfitting
	- 1 way to combat overfitting is weight decay aka L2 Regularization 
	- For tab and collab data sets, hard to determine which wd to use. So start with wd=0.1 then divide by 10 a couple of times. See what gives best result.
- Note: Bias is needed in a NN because if the input to the layer is 0, then that 0 will be carried on throughout the neural network, messing up the predictions. 
	Bias will shift away from this 0 so that the NN can continue to learn.

- Note: For pytorch a method that ends in _ will change in place the tensor that its being applied to 
- Note: Can put ?? after class,method,function names in python/jupyter notebooks to see the source code (maybe this just works for pytorch - will have to check) 
- Note: **kwargs in python means "put any additionaly keyword arguments into a dict called kwargs


Embeddings
- NLP Embeddings:
	- NLPs create embeddings matrix's for documents. Each token has an associated vector and the matrix is all those vectors concat tg row after row corresponding to the order of the tokens

- Tabular Embeddings:
	- Embeddings can also be created from tabular data to learn latent factors.

- We can use embeddings to learn latent factors between different things. Pretty cool ngl.
	


